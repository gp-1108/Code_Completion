<p align="center">
  <img src="https://img.icons8.com/?size=512&id=55494&format=png" width="20%" alt="<code>❯ REPLACE-ME</code>-logo">
</p>
<p align="center">
    <h1 align="center">Analyzing LLM Code Completions</h1>
</p>
<p align="center">
    <em>Empowering Code Creation Through Intelligent Evaluation!</em>
</p>
<p align="center">
	<!-- Shields.io badges disabled, using skill icons. --></p>
<p align="center">
		<em>Built with the tools and technologies:</em>
</p>
<p align="center">
	<a href="https://skillicons.dev">
		<img src="https://skillicons.dev/icons?i=md,py">
	</a></p>

<br>

##### 🔗 Table of Contents

- [📍 Overview](#-overview)
- [👾 Features](#-features)
- [📂 Repository Structure](#-repository-structure)
- [🧩 Modules](#-modules)
- [🚀 Getting Started](#-getting-started)
    - [🔖 Prerequisites](#-prerequisites)
    - [📦 Installation](#-installation)
    - [🤖 Usage](#-usage)
- [🎗 License](#-license)

---

## 📍 Overview

This project aims to analyze code completions generated by large language models (LLMs) to evaluate their quality and effectiveness. By leveraging deep learning models and natural language processing techniques, the project provides a comprehensive framework for generating and evaluating code snippets. A small dataset of code completions was created and a deep seek model was used to generate code completions. The generated code snippets were then evaluated manually to assess their quality and correctness. The project includes Jupyter notebooks for code generation and evaluation, as well as Python scripts for manual evaluation and analysis. Afterwards, a detailed analysis was performed to identify which metrics are most important for evaluating code completions and to determine the strengths and weaknesses of the generated code snippets.

More practically you will find the following functionalities:
- ```Code snippets generation``` using a deep seek model. Take a look at the ```generation``` folder for this bit of the project. Basically you can generate pairs of code snippets with the associated code completion given a github project.
- ```Code snippets evaluation``` using either a manual evaluation script and/or a suite of automated metrics . Take a look at the ```evaluation``` folder for this bit of the project. Basically you can evaluate the generated code completions and see how well they perform.


If you would like to take a look at my generated dataset you can find it here:
1. ```evaluation/model_outputs.json``` - This file contains the generated code completions.
2. ```evaluation/manual_scores.json``` - This file contains the manual evaluation scores of the generated code completions.

---

## 👾 Features

|    |   Feature         | Description |
|----|-------------------|---------------------------------------------------------------|
| 🔩 | **Code Quality**  | The codebase adheres to PEP 8 standards, emphasizing readability and maintainability. It includes modular functions and clear variable names, making it easy for contributors to understand and extend. |
| 📄 | **Documentation** | Documentation is comprehensive, with well-structured Jupyter notebooks explaining the architecture, components, and usage. Each file has relevant comments, enhancing user experience and ease of navigation. |
| 🔌 | **Integrations**  | Key integrations include deep learning libraries like Torch and natural language processing tools like Transformers, supporting functionality for model training and evaluation. GitPython is integrated for repository management. |
| 🧩 | **Modularity**    | The codebase is highly modular; individual components like evaluation scripts and generation utilities are encapsulated, promoting reusability and ease of testing. This structure allows for easier updates and feature additions. |
| 📦 | **Dependencies**  | Key libraries include 'torch' for deep learning, 'transformers' for NLP tasks, and 'GitPython' for managing repository interactions, ensuring essential functionality across various development aspects. |

---

## 📂 Repository Structure

```sh
└── /
    ├── README.md
    ├── evaluation
    │   ├── analysis.ipynb
    │   ├── images
    │   ├── manual_evaluation_script.py
    │   ├── manual_scores.json
    │   └── model_outputs.json
    ├── generation
    │   ├── Code_generation_LLM.ipynb
    │   └── utils
    └── requirements.txt
```

---

## 🧩 Modules

<details closed><summary>.</summary>

| File | Summary |
| --- | --- |
| [requirements.txt](requirements.txt) | Defines essential dependencies for the repository, ensuring compatibility and functionality across various components. This configuration supports features like Git integration, deep learning via Torch, and natural language processing through Transformers, which are pivotal for the evaluation and generation modules within the overall architecture. |

</details>

<details closed><summary>evaluation</summary>

| File | Summary |
| --- | --- |
| [model_outputs.json](evaluation/model_outputs.json) | The `evaluation/model_outputs.json` file serves a crucial role in the repositorys architecture by storing the results of model evaluations concerning code generation. Specifically, it contains a list of JSON objects that capture the various components of generated code snippets, including the prefix, generated code, correct middle portions, and suffixes. This file allows for the systematic assessment of the performance of the code generation model implemented in the repository, thereby facilitating insights into its accuracy and efficacy in producing valid code structures. By keeping track of the expected and actual outputs, this file enhances the framework for evaluating and refining the model, ensuring that the overall goal of producing reliable code generation tools is met. |
| [manual_evaluation_script.py](evaluation/manual_evaluation_script.py) | Facilitates an interactive platform for evaluating code samples through a user-friendly graphical interface. Provides features for loading JSON data, rating samples with star metrics, navigating between samples, and saving evaluation scores, thereby enhancing the assessment of generated code quality within the repositorys architecture. |
| [analysis.ipynb](evaluation/analysis.ipynb) | The code file in question serves a pivotal role within the overall architecture of the repository. Its main purpose is to analyze the results gotten from the generated samples and evaluated them. |
| [manual_scores.json](evaluation/manual_scores.json) | The `evaluation/manual_scores.json` file serves as a critical component of the repository, enabling the collection and storage of evaluation metrics related to generated code outputs. This file captures user ratings across multiple dimensions, including satisfaction, similarity, completeness, and the presence of errors in the evaluated code snippets. By providing structured feedback for various code segments—namely their prefixes and suffixes—the file facilitates a quantitative assessment of the effectiveness of code generation processes. In essence, the `manual_scores.json` file is used to store the final manual ratings of the generated code snippets. |

</details>

<details closed><summary>generation</summary>

| File | Summary |
| --- | --- |
| [Code_generation_LLM.ipynb](generation/Code_generation_LLM.ipynb) | The `generation/Code_generation_LLM.ipynb` file is designed to facilitate the generation of code using the deep seek model. It operates on a custom dataset, enabling users to create code examples that can later be evaluated using tools and scripts located in the `evaluation` directory.|

</details>

<details closed><summary>generation.utils</summary>

| File | Summary |
| --- | --- |
| [RepoExtractor.py](generation/utils/RepoExtractor.py) | Facilitates the cloning of repositories from specified URLs, processes files with certain extensions, and extracts their contents. By utilizing extension-specific handlers, it efficiently manages various file types, particularly Jupyter notebooks, making it a crucial component for the generation modules ability to analyze and utilize code resources effectively. |
| [CodeDataset.py](generation/utils/CodeDataset.py) | Facilitates the generation of code completion examples by extracting segments from provided code file contents, ensuring variability within specified length constraints. This utility enhances the dataset for the code generation tasks in the repository, supporting model training and evaluation in the overall architecture. |

</details>

---

## 🚀 Getting Started

### 🔖 Prerequisites

**Python**: `version 3.10.x`
**Dependencies**: `requirements.txt`

### 📦 Installation

Build the project from source:

1. Clone the  repository:
```sh
❯ git clone https://github.com/gp-1108/Code_Completion
```

2. Install the required dependencies:
```sh
❯ pip install -r requirements.txt
```

### 🤖 Usage

The project can be either used to generated code completions given a github folder or to evaluate the generated code completions.

#### Code Generation

To generate code completions, take a look at the ```generation/Code_generation_LLM.ipynb``` notebook. This notebook will guide you through the process of generating code completions using a deep seek model.

#### Code Evaluation

To evaluate the generated code completions, take a look at the ```evaluation/manual_evaluation_script.py``` script. This script will guide you through the process of evaluating the generated code completions.
Once you have evaluated the code completions you can have a look at your results in the ```evaluation/Code_generation_LLM.ipynb``` notebook.

If you just want to see the already computed results, you can take a look at the ```evaluation/analysis.ipynb``` notebook without touching anything.

---

## 🎗 License

This project is protected under the [GNU-AGPL 3.0](https://choosealicense.com/licenses/agpl-3.0/) License. 

---